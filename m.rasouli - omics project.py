# -*- coding: utf-8 -*-
"""21bahman.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-8QrOh4rsc_rj5lspa5jiZWHljSAF3Gw
"""

#Importing libraries

#----
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import seaborn as sns
from numpy import mean, std
from scipy.stats import t
#Feature selection 
from sklearn.feature_selection import SelectKBest, mutual_info_classif, f_classif,chi2
from sklearn.feature_selection import RFE
from sklearn.decomposition import PCA

#Classifires
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from sklearn.model_selection import ShuffleSplit
from sklearn.utils import resample
from sklearn.metrics import classification_report
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.metrics import confusion_matrix

#Regression
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LassoCV
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso
from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

#Import datasets
rna = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Dataset/rna.csv')
meta = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Dataset/meta.csv')

#Scaling RNA seq dataset
Patient_ID = rna['Patient_ID']
rna_num = rna.drop(["Patient_ID"],axis=1)
scaler = MinMaxScaler()
sc_rna = scaler.fit_transform(rna_num)
scaled_rna = pd.DataFrame (sc_rna, columns = rna_num.columns)
scaled_rna ['Patient_ID'] = Patient_ID

#Scaling RNA seq dataset
drop = ['BMI_surg','Age']
Meta = meta.drop(columns=drop)
META = meta[drop]
data_to_scale = META.values
scaled_meta = pd.DataFrame (scaler.fit_transform(META), columns= META.columns)
Meta ['BMI_surg'] = scaled_meta['BMI_surg'] 
Meta ['Age'] = scaled_meta['Age']

#Integration of RNAseq and Metadata
df = pd.merge(scaled_rna, Meta, on = "Patient_ID")
df = df.set_index('Patient_ID')

#Encoding categorical variables
df['SEX'].replace(['Female', 'Male'],[0, 1], inplace=True)
df['Diabet'].replace(['Non Diabetic', 'Diabetic'],[0, 1], inplace=True)
df['Simplified_class'].replace(['Normal','Advanced_fibrosis','Non_advanced_Fibrosis'], [0,1,2], inplace = True)
df = df.drop('Run', axis=1)

#Define X and y
X = df.drop(['Simplified_class'], axis = 1)
y = df['Simplified_class']

# plot a histogram
plt.hist(X)
plt.show()

#**********************************FS**********************************
#************************ Filter based FS ******************************* 
#************************ f_classif **********************************

# Define a range of values for K
k_range = np.arange(1,16000, 50)

# Store the cross-validation scores for each value of K
scores = []
for k in k_range:
    selector = SelectKBest(f_classif, k=k)
    X_new = selector.fit_transform(X, y)
    model = LogisticRegression(max_iter=17000)
    score = np.mean(cross_val_score(model, X_new, y, cv=5))
    scores.append(score)

#Find the value of K that results in the highest accuracy
best_k = k_range[np.argmax(scores)]
print("The optimum value of K is:", best_k)

#Set 401 as optimum feature number in f_classif F.S method -> acc = 0.872
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Select the top k features using f_classif

selector = SelectKBest(f_classif, k=best_k)
X_train_kbest = selector.fit_transform(X_train, y_train)
X_test_kbest = selector.transform(X_test)

# Train a logistic regression model on the selected features
model = LogisticRegression(max_iter=17000)
model.fit(X_train_kbest, y_train)

# Evaluate the accuracy of the model on the test data
y_pred_fclassif = model.predict(X_test_kbest)
acc = accuracy_score(y_test, y_pred_fclassif)
print(f'Accuracy with {k} best features: {acc:.3f}')


# Plot the results
plt.plot(k_range, scores)
plt.xlabel('Value of K for SelectKBest')
plt.ylabel('Cross-Validated Accuracy')
plt.show()

# Check if is there any metadata feature 
f_class = pd.DataFrame(X_train_kbest, columns=X_train.columns[selector.get_support()])
for i in f_class:
  if i in Meta.columns:
    print(i)

#************************ chi2 **********************************

# Define a range of values for K
k_range = np.arange(1,16000, 50)

# Store the cross-validation scores for each value of K
scores = []
for k in k_range:
    selector = SelectKBest(score_func=chi2, k=k)
    X_new = selector.fit_transform(X, y)
    model = LogisticRegression(max_iter=17000)
    score = np.mean(cross_val_score(model, X_new, y, cv=3))
    scores.append(score)

#Find the value of K that results in the highest accuracy
best_k = k_range[np.argmax(scores)]
print("The optimum value of K is:", best_k)
#The optimum value of K is: 6301

# Select the top k features using f_classif
k = 6290
selector_chi = SelectKBest(score_func=chi2, k=best_k)
X_train, X_test, y_train,y_test = train_test_split(X,y, test_size=0.2, random_state=42)
X_train_chi = selector_chi.fit_transform(X, y)
X_test_kbest = selector_chi.transform(X_test)

# Train a logistic regression model on the selected features
model = LogisticRegression(max_iter=17000)
model.fit(X_train_kbest, y_train)

# Evaluate the accuracy of the model on the test data
y_pred_chi = model.predict(X_test_kbest)
acc = accuracy_score(y_test, y_pred_fclassif)
print(f'Accuracy with {k} best features: {acc:.3f}')

# Select the top k features using f_classif
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Select the top k features using f_classif
k =1879
selector = SelectKBest(score_func=chi2, k=k)

X_train_kbest = selector.fit_transform(X_train, y_train)
X_test_kbest = selector.transform(X_test)

# Train a logistic regression model on the selected features
model = LogisticRegression(max_iter=17000)
model.fit(X_train_kbest, y_train)

# Evaluate the accuracy of the model on the test data
y_pred_fclassif = model.predict(X_test_kbest)
acc = accuracy_score(y_test, y_pred_fclassif)
print(f'Accuracy with {k} best features: {acc:.3f}'


# Check if is there any metadata feature
chi2 = pd.DataFrame(X_train_chi, columns=X_train.columns[selector_chi.get_support()])
for i in chi2:
  if i in Meta.columns:
    print(i)

#************************ Mutal information **********************************
#FS : Mutal information : 1401 -> 0.897

# Define a range of values for k
k_range = range(1300,1401,10)


scores = []

# Loop over the values of k
for k in k_range:
    selector = SelectKBest(mutual_info_classif, k=k)
    X_new = selector.fit_transform(X, y)

    #Train a logistic regression model using cross-validation
    clf = LogisticRegression(max_iter = 17000)
    cv_scores = cross_val_score(clf, X_new, y, cv=5)

    #Store the mean cross-validation score
    mean_score = np.mean(cv_scores)
    scores.append(mean_score)

#Best k
best_k = k_range[np.argmax(scores)]
print("Best k: ", best_k)


selector = SelectKBest(mutual_info_classif, k=best_k)
X_new = selector.fit_transform(X, y)

#Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=42)

#Train a logistic regression model
clf = LogisticRegression()
clf.fit(X_train, y_train)

# Evaluate the accuracy of the model on the testing data
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy of the model: ", accuracy)

# Plot the results
plt.plot(k_range, scores)
plt.xlabel('Value of K for SelectKBest')
plt.ylabel('Cross-Validated Accuracy')
plt.show()


# Use the best value of k to perform feature selection and evaluate the accuracy of the model: 1350 -> 89%
selector_mi = SelectKBest(mutual_info_classif, k=1350)
X_new_mi = selector_mi.fit_transform(X, y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_new_mi, y, test_size=0.2, random_state=42)

# Train a logistic regression model on the training data
clf = LogisticRegression(max_iter=17000)
clf.fit(X_train, y_train)

# Evaluate the accuracy of the model on the testing data
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy of the model: ", accuracy)

# Create a DataFrame with the selected features and their names
mi = pd.DataFrame(X_new_mi, columns=X.columns[selector_mi.get_support()])
for i in mi.columns:
  if i in Meta.columns:
    print(i, 'Selected')

#Use the best value of k to perform feature selection and evaluate the accuracy of the model: 1350 -> 89%
selector_mi = SelectKBest(mutual_info_classif, k=1350)
X_new_mi = selector_mi.fit_transform(X, y)

#Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_new_mi, y, test_size=0.2, random_state=42)

#Train a logistic regression model on the training data
clf = LogisticRegression(max_iter=17000)
clf.fit(X_train, y_train)

#Evaluate the accuracy of the model on the testing data
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy of the model: ", accuracy)

#Create new dataframe with the selected features and their names
mi = pd.DataFrame(X_new_mi, columns=X.columns[selector_mi.get_support()])
for i in mi.columns:
  if i in Meta.columns:
    print(i, 'Selected')

filter = pd.DataFrame({'methods':['mutual_info_classif','f_classif', 'Chi2' ],
                   'accuracy':[0.897,0.872, 0.845],
                    'feature selected': [1350, 401, 1850],
                   'metadata features':['N/A','N/A', 'Diabet']})


wrapem = pd.DataFrame({'methods':['Random Forest','RFE', 'Lasso' ],
                   'accuracy':[0.947,0.793, None],
                   'MSE':[None, None, 0.20],
                    'feature selected': [330, 245, 1230]})

sns.barplot(x='methods', y='accuracy', data=filter)
plt.xlabel("Feature Selection Method")
plt.ylabel("Accuracy")
plt.title("Accuracy of Different Feature Selection Methods")
plt.show()

#***************************** FS ******************************
#************************ Wrapper based FS ************************** 
#************************ RFE *********************************

#split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(mi, y, test_size=0.2)


#initialize the model and RFE
dt = DecisionTreeClassifier()
rfe = RFE(dt, n_features_to_select=180 )

#fit the RFE to the training data
rfe = rfe.fit(X_train, y_train)

#make predictions using the test data
y_pred = rfe.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(accuracy)



#***************************** FS ******************************
#************************ Embedded based FS ************************** 
#************************ Lasso *********************************


X_train, X_test, y_train, y_test = train_test_split(mi, y, test_size=0.2, random_state=42)

#Create a LassoCV 
alphas = np.logspace(-19, 10, 20)
lasso_cv = LassoCV(alphas=alphas, cv=5, random_state=42,max_iter=10000)

#Fit the model 
model = lasso_cv.fit(X_train, y_train)

#Predict on the test data
y_pred = model.predict(X_test)

#Compute MSE
mse = mean_squared_error(y_test, y_pred)

#Plot the mse for each alpha
mse_path = model.mse_path_
plt.plot(model.alphas_, mse_path.mean(axis=-1), label='Mean Squared Error')

#Plot the minimum mean squared error
min_idx = np.argmin(mse_path.mean(axis=-1))
plt.axvline(model.alphas_[min_idx], color='r', linestyle='--', label='Minimum MSE')

plt.xscale('log')
plt.xlabel('Alpha')
plt.ylabel('Mean Squared Error')
plt.legend()
plt.show()

# Print the optimum alpha
print('Optimum Alpha:', lasso_cv.alpha_)
print('Mean Squared Error:', mse)

#************************ RandomForest *********************************

#Embedded FS: RF -> Accuracy of the model:  0.9230769230769231
#Train a RandomForestClassifier 
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(mi, y)

#Use the trained classifier to select the most important features
model = SelectFromModel(clf, prefit=True)
X_new = model.transform(mi)

#training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=42)

# Train a logistic regression
clf = LogisticRegression(max_iter=17000)
clf.fit(X_train, y_train)

#Evaluate the accuracy 
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy of the model: ", accuracy)


rf_selected = pd.DataFrame(X_new, columns=mi.columns[model.get_support()])

# Apply PCA
pca = PCA(n_components=45)
pca_selected = pca.fit_transform(rf_selected)
result = pd.DataFrame(data = pca_selected)

# Plot the first two principal components
fig, ax = plt.subplots()
ax.scatter(result.iloc[:,0], result.iloc[:,1], s=30, c='red', alpha=0.5)
ax.set_title("PCA Output Scatter Plot")
ax.set_xlabel("PC1")
ax.set_ylabel("PC2")
plt.show()

#Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(result, y, test_size=0.2, random_state=42)

#*************************** Classifire **********************
#*************************** KNN *****************************

#Define the KNN classifier
knn = KNeighborsClassifier()

#the grid of hyperparameters to search
param_grid = {'n_neighbors': [3, 4,5,6, 7,8], 'weights': ['uniform', 'distance'], 'metric': ['euclidean', 'manhattan']}

grid_search = GridSearchCV(knn, param_grid, cv=5)
grid_search.fit(X_train, y_train)
best_params = grid_search.best_params_

#Train the classifier with the best hyperparameters
knn = KNeighborsClassifier(n_neighbors=best_params['n_neighbors'], weights=best_params['weights'], metric=best_params['metric'])
knn.fit(X_train, y_train)

#Make predictions 
y_pred = knn.predict(X_test)
#Best params
print(grid_search.best_params_)


#performance of the classifier
accuracy_knn = accuracy_score(y_test, y_pred)
report_knn = classification_report(y_test, y_pred)
print(f'Accuracy: {accuracy_knn:.2f}\nClassification report:\n{report_knn}')


cm = confusion_matrix(y_test, y_pred)

#[lot the confusion matrix
plt.imshow(cm, cmap='Blues', interpolation='nearest')
plt.title('KNN without Metadata')
plt.colorbar()
tick_marks = np.arange(len(np.unique(y_test)))
plt.xticks(tick_marks, np.unique(y_test), rotation=45)
plt.yticks(tick_marks, np.unique(y_test))
plt.tight_layout()
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

#*************************** LogisticRegression *****************************
# Define the hyperparameters to be tuned using cross-validation
hyperparams = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}

#to select the best hyperparameters
clf = GridSearchCV(LogisticRegression(max_iter = 20000), hyperparams, cv=5)
clf.fit(X_train, y_train)


logistic_clf = LogisticRegression(C=clf.best_params_['C'])
logistic_clf.fit(X_train, y_train)

# performance of the classifier on the testing set 
y_pred = logistic_clf.predict(X_test)
report_log = classification_report(y_test, y_pred)

print(clf.best_params_)
# Evaluate the performance of the classifier
accuracy_log = accuracy_score(y_test, y_pred)
report_log = classification_report(y_test, y_pred)
print(f'Accuracy: {accuracy_log:.2f}\nClassification report:\n{report_log}')

cm = confusion_matrix(y_test, y_pred)

#Plot the confusion matrix
plt.imshow(cm, cmap='Blues', interpolation='nearest')
plt.title('logistic clf without Metadata')
plt.colorbar()
tick_marks = np.arange(len(np.unique(y_test)))
plt.xticks(tick_marks, np.unique(y_test), rotation=45)
plt.yticks(tick_marks, np.unique(y_test))
plt.tight_layout()
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

#*************************** GaussianNB *****************************

# find hyperparameters to be tuned 
hyperparams = {'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]}

# select the best hyperparameters 
clf = GridSearchCV(GaussianNB(), hyperparams, cv=5)
clf.fit(X_train, y_train)

#Train the Gaussian Naive classifier 
gnb_clf = GaussianNB(var_smoothing=clf.best_params_['var_smoothing'])
gnb_clf.fit(X_train, y_train)

#Evaluate the performance 
y_pred = gnb_clf.predict(X_test)
report_naive = classification_report(y_test, y_pred)

#best params
print(clf.best_params_)


# Evaluate the performance of the classifier
accuracy_naive = accuracy_score(y_test, y_pred)
report_naive = classification_report(y_test, y_pred)
print(f'Accuracy: {accuracy_naive:.2f}\nClassification report:\n{report_naive}')

cm = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix 
plt.imshow(cm, cmap='Blues', interpolation='nearest')
plt.title('Naive without Metadata')
plt.colorbar()
tick_marks = np.arange(len(np.unique(y_test)))
plt.xticks(tick_marks, np.unique(y_test), rotation=45)
plt.yticks(tick_marks, np.unique(y_test))
plt.tight_layout()
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

#*************************** SVM *****************************

# hyperparameters 
hyperparams = {
    'C': [0.1, 1, 10, 100],
    'gamma': [0.01, 0.1, 1, 10]
}

#best hyperparameters for the SVM classifier
clf = GridSearchCV(SVC(kernel='rbf'), hyperparams, cv=5)
clf.fit(X_train, y_train)


print(clf.best_params_)

#Train SVM 
svm_clf = SVC(kernel='rbf', C=clf.best_params_['C'], gamma=clf.best_params_['gamma'])
svm_clf.fit(X_train, y_train)

# Evaluate the performance 
y_pred = svm_clf.predict(X_test)
accuracy_svm = accuracy_score(y_test, y_pred)
report_svm = classification_report(y_test, y_pred)
print(f'Accuracy: {accuracy_svm:.2f}\nClassification report:\n{report_svm}')

cm = confusion_matrix(y_test, y_pred)

# Plot the confusion matri
plt.imshow(cm, cmap='Blues', interpolation='nearest')
plt.title('SVM without Metadata')
plt.colorbar()
tick_marks = np.arange(len(np.unique(y_test)))
plt.xticks(tick_marks, np.unique(y_test), rotation=45)
plt.yticks(tick_marks, np.unique(y_test))
plt.tight_layout()
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

#*************************** RandomForestClassifier *****************************

hyperparams = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 5, 10, 20],
    'min_samples_split': [2, 5, 10]
}

clf = GridSearchCV(RandomForestClassifier(), hyperparams, cv=5)
clf.fit(X_train, y_train)


print(clf.best_params_)

# Train the random foresst
rf_clf = RandomForestClassifier(n_estimators=clf.best_params_['n_estimators'], 
                                 max_depth=clf.best_params_['max_depth'], 
                                 min_samples_split=clf.best_params_['min_samples_split'])
rf_clf.fit(X_train, y_train)

# Evaluate the performance 
y_pred = rf_clf.predict(X_test)
accuracy_rf = accuracy_score(y_test, y_pred)
report_rf = classification_report(y_test, y_pred)
print(f'Accuracy: {accuracy_rf:.2f}\nClassification report:\n{report_rf}')

cm = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix 
plt.imshow(cm, cmap='Blues', interpolation='none')
plt.title('RF without Metadata')
plt.colorbar()
tick_marks = np.arange(len(np.unique(y_test)))
plt.xticks(tick_marks, np.unique(y_test), rotation=45)
plt.yticks(tick_marks, np.unique(y_test))
plt.tight_layout()
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

#Find repeatability
# Cross-validation
cv = ShuffleSplit(n_splits=30, test_size=0.3, random_state=42)
scores = cross_val_score(svm_clf, result, y, scoring='accuracy', cv=cv)

print('Cross-validation accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))

# Plot the scores as a bar plot
plt.bar(range(len(scores)), scores)
plt.xlabel('Iteration')
plt.ylabel('Accuracy')
plt.show()

#-------------------------------------Question2---------------------------------


#including Metadata features:
result2 = result.copy()

#get scaled and standard meta from df dataset
st_meta =df.iloc[:, -5:]

result2.columns = ['PC' + str(i+1) for i in range(len(result2.columns))]


# merge the two DataFrames using a cross join
st_meta['dummy'] = np.arange(1,192)
result2['dummy'] = np.arange(1,192)
merged_df = pd.merge(st_meta, result2, on='dummy')

# drop the dummy and label column
merged_df = merged_df.drop(columns='Simplified_class')
merged_df = merged_df.drop(columns='dummy')
merged_df.head()

#Split into training and testing sets for SVM machine
X_train2, X_test2, y_train2, y_test2 = train_test_split(merged_df,y, test_size = 0.2, random_state = 42)

#**************  Classifires with Metadata features  ***************************

#*******************************  KNN  *****************************************

# Define the KNN classifier
knn = KNeighborsClassifier()

#Find best hyperparams
param_grid = {'n_neighbors': [3, 4,5,6, 7,8], 'weights': ['uniform', 'distance'], 'metric': ['euclidean', 'manhattan']}
grid_search = GridSearchCV(knn, param_grid, cv=5)
grid_search.fit(X_train2, y_train2)
best_params = grid_search.best_params_

#Train the classifier with the best hyperpars
knn = KNeighborsClassifier(n_neighbors=best_params['n_neighbors'], weights=best_params['weights'], metric=best_params['metric'])
knn.fit(X_train2, y_train2)

#predictions on the test set
y_pred2 = knn.predict(X_test2)

#Print best params
print(grid_search.best_params_)


#Evaluate the performance of the classifier
accuracy_knn = accuracy_score(y_test2, y_pred2)
report_knn = classification_report(y_test2, y_pred2)
print(f'Accuracy: {accuracy_knn:.2f}\nClassification report:\n{report_knn}')

cm = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix 
plt.imshow(cm, cmap='binary', interpolation='nearest')
plt.title('KNN with Metadata')
plt.colorbar()
tick_marks = np.arange(len(np.unique(y_test)))
plt.xticks(tick_marks, np.unique(y_test), rotation=45)
plt.yticks(tick_marks, np.unique(y_test))
plt.tight_layout()
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

#***************************  Logistic Regression  *****************************

#Best hyperparams
hyperparams = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}
clf = GridSearchCV(LogisticRegression(max_iter=15500), hyperparams, cv=5)
clf.fit(X_train2, y_train2)

#Train the logistic classifier on the training set using the selected hyperparams
logistic_clf = LogisticRegression(C=clf.best_params_['C'])
logistic_clf.fit(X_train2, y_train2)

print(clf.best_params_)

#Evaluate the performance
y_pred2 = logistic_clf.predict(X_test2)

accuracy_log = accuracy_score(y_test2, y_pred2)
report_log = classification_report(y_test2, y_pred2)
print(f'Accuracy: {accuracy_log:.2f}\nClassification report:\n{report_log}')

cm = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix 
plt.imshow(cm, cmap='binary', interpolation='nearest')
plt.title('Logistic clf with Metadata')
plt.colorbar()
tick_marks = np.arange(len(np.unique(y_test)))
plt.xticks(tick_marks, np.unique(y_test), rotation=45)
plt.yticks(tick_marks, np.unique(y_test))
plt.tight_layout()
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

#***************************  Gaussian Naive Bayes  *****************************

#Best hyperparams
hyperparams = {'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]}
clf = GridSearchCV(GaussianNB(), hyperparams, cv=5)
clf.fit(X_train2, y_train2)

#Train the Gaussian Naive Bayes classifier
gnb_clf = GaussianNB(var_smoothing=clf.best_params_['var_smoothing'])
gnb_clf.fit(X_train2, y_train2)


#best params
print(clf.best_params_)

#Evaluate the performance 
y_pred2 = gnb_clf.predict(X_test2)
accuracy_naive = accuracy_score(y_test2, y_pred2)
report_naive = classification_report(y_test2, y_pred2)
print(f'Accuracy: {accuracy_naive:.2f}\nClassification report:\n{report_naive}')

cm = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix 
plt.imshow(cm, cmap='binary', interpolation='nearest')
plt.title('Naive with Metadata')
plt.colorbar()
tick_marks = np.arange(len(np.unique(y_test)))
plt.xticks(tick_marks, np.unique(y_test), rotation=45)
plt.yticks(tick_marks, np.unique(y_test))
plt.tight_layout()
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

#***************************  SVM  *****************************
# Define the hyperparameters 
hyperparams = {
    'C': [0.1, 1, 10, 100],
    'gamma': [0.01, 0.1, 1, 10]
}
clf = GridSearchCV(SVC(kernel='rbf'), hyperparams, cv=5)
clf.fit(X_train2, y_train2)

# Print the hyperparameters
print(clf.best_params_)

# Train the SVM classifier 
svm_clf = SVC(kernel='rbf', C=clf.best_params_['C'], gamma=clf.best_params_['gamma'])
svm_clf.fit(X_train2, y_train2)

# Evaluate the performance 
y_pred = svm_clf.predict(X_test2)
accuracy_svm = accuracy_score(y_test2, y_pred2)
report_svm = classification_report(y_test2, y_pred2)
print(f'Accuracy: {accuracy_svm:.2f}\nClassification report:\n{report_svm}')

cm = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix 
plt.imshow(cm, cmap='binary', interpolation='nearest')
plt.title('SVM with Metadata')
plt.colorbar()
tick_marks = np.arange(len(np.unique(y_test)))
plt.xticks(tick_marks, np.unique(y_test), rotation=45)
plt.yticks(tick_marks, np.unique(y_test))
plt.tight_layout()
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

#***************************  RandomForestClassifier  *****************************
# Define the hyperparams
hyperparams = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 5, 10, 20],
    'min_samples_split': [2, 5, 10]
}
clf = GridSearchCV(RandomForestClassifier(), hyperparams, cv=5)
clf.fit(X_train2, y_train2)

# Print the hyperparameters selected
print(clf.best_params_)

# Train the random forest classifier
rf_clf = RandomForestClassifier(n_estimators=clf.best_params_['n_estimators'], 
                                 max_depth=clf.best_params_['max_depth'], 
                                 min_samples_split=clf.best_params_['min_samples_split'])
rf_clf.fit(X_train2, y_train2)

# Evaluate the performance 
y_pred = rf_clf.predict(X_test2)
accuracy_rf = accuracy_score(y_test2, y_pred2)
report_rf = classification_report(y_test2, y_pred2)
print(f'Accuracy: {accuracy_rf:.2f}\nClassification report:\n{report_rf}')


cm = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix
plt.imshow(cm, cmap='binary', interpolation='nearest')
plt.title('RF with Metadata')
plt.colorbar()
tick_marks = np.arange(len(np.unique(y_test)))
plt.xticks(tick_marks, np.unique(y_test), rotation=45)
plt.yticks(tick_marks, np.unique(y_test))
plt.tight_layout()
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

#------------------------------------------Question3-----------------------------

result3 = result.copy()
#get scaled and standard meta from df dataset
st_meta =df.iloc[:, -5:]

result3.columns = ['PC' + str(i+1) for i in range(len(result3.columns))]
# add a dummy column to each DataFrame with the same value
st_meta['dummy'] = np.arange(1,192)
result3['dummy'] = np.arange(1,192)


# merge the two DataFrames using a cross join
sub_df = pd.merge(result3,st_meta,  on='dummy')

# drop the dummy and label column

sub_df = sub_df.drop(columns='dummy')

sub_df['Simplified_class'].value_counts()

#1st Classification
X = sub_df.drop(['Simplified_class'],axis = 1)  
y = sub_df['Simplified_class']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Split the data into training and testing sets

#Train / test split
binary_model = LogisticRegression()
binary_model.fit(X_train, y_train)

#Evaluate 1st clf
binary_predictions = binary_model.predict(X_test)
binary_accuracy = accuracy_score(y_test, binary_predictions)
print(f"First classification accuracy: {binary_accuracy}")
report_clf1 = classification_report(y_test, binary_predictions)
print(report_clf1)

#2nd clf
from sklearn.ensemble import RandomForestClassifier


diseased_mask = binary_predictions != 0  # Select samples that are predicted as diseased (except Normals)
X_diseased = X_test[diseased_mask]  
y_diseased = y_test[diseased_mask]


multi_class_model = RandomForestClassifier()
multi_class_model.fit(X_diseased, y_diseased)

# Evaluate the 2nd classification model
multi_class_predictions = multi_class_model.predict(X_diseased)
multi_class_accuracy = accuracy_score(y_diseased, multi_class_predictions)
print(f"Sub-disease classification accuracy: {multi_class_accuracy}")

report_clf2 = classification_report(y_diseased, multi_class_predictions)
print(report_clf2)

cm = confusion_matrix(y_test, binary_predictions)

# Plot the confusion matrix as a heatmap
plt.imshow(cm, cmap='Blues', interpolation='nearest')
plt.title('First machine: Logistic reg')
plt.colorbar()
tick_marks = np.arange(len(np.unique(y_test)))
plt.xticks(tick_marks, np.unique(y_test), rotation=45)
plt.yticks(tick_marks, np.unique(y_test))
plt.tight_layout()
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()
cm = confusion_matrix(y_diseased, multi_class_predictions)

# Plot the confusion matrix 
plt.imshow(cm, cmap='Greens', interpolation='nearest')
plt.title('Second machine: RF')
plt.colorbar()
tick_marks = np.arange(len(np.unique(y_diseased)))
plt.xticks(tick_marks, np.unique(y_diseased), rotation=45)
plt.yticks(tick_marks, np.unique(y_diseased))
plt.tight_layout()
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

#-----------------------------------Question4-----------------------------------
#************************ Regression for predicting age  ***********************


#get scaled and standard meta from df dataset
st_meta =df.iloc[:, -5:]
result3 = result.copy()
result3.columns = ['PC' + str(i+1) for i in range(len(result3.columns))]
# add a dummy column to each DataFrame with the same value
st_meta['dummy'] = np.arange(1,192)
result3['dummy'] = np.arange(1,192)

# merge the two DataFrames using a cross join
sub_df = pd.merge(result3,st_meta,  on='dummy')

# drop the dummy and label column
sub_df = sub_df.drop(columns='dummy')
#Define X and y 
X = sub_df.drop(['Age'],axis=1)
y = sub_df['Age']

#********************** Linear Regression  *******************************

#Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Define LR model
reg = LinearRegression().fit(X_train, y_train)
reg.score(X_train, y_train)

y_pred = reg.predict(X_test)


# Plot the predicted vs actual values
fig, ax = plt.subplots()
ax.scatter(y_pred, y_test, edgecolors=(0, 0, 0))
ax.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=3)
ax.set_xlabel('Actual')
ax.set_ylabel('Predicted')
ax.set_title('Linear Regression Model for male and female')

plt.xlim([0,1])
plt.ylim([-0.1,1])

plt.show()

# Calculation of Mean Squared Error (MSE)
mse = mean_squared_error(y_test,y_pred)
print('MSE:',mse)

#**************  Find minimum impactful features for Reduced df  ****************
# Initialize Lasso model

lasso = Lasso(alpha=0.1)
lasso.fit(X, y)

print("Selected features:", np.where(lasso.coef_ != 0)[0])

#Determine minimum number of features needed for a good model
alphas = np.logspace(-4, 1, 10)
scores = []
for alpha in alphas:
    lasso = Lasso(alpha=alpha)
    lasso.fit(X, y)
    scores.append(lasso.score(X, y))

optimal_alpha = alphas[np.argmax(scores)]
lasso = Lasso(alpha=optimal_alpha)
lasso.fit(X, y)
num_features = np.count_nonzero(lasso.coef_)
print("Minimum number of features for a good model:", num_features)

#****************  minimum impactful features for original df ******************
X = df.drop(['Age'],axis = 1)
y = df['Age']
# Initialize Lasso model 
lasso = Lasso(alpha=0.1, max_iter=27000)
lasso.fit(df, y)

# Print selected features
print("Selected features:", np.where(lasso.coef_ != 0)[0])

# Determine minimum number of features needed for a good model
alphas = np.logspace(-4, 1, 100)
scores = []
for alpha in alphas:
    lasso = Lasso(alpha=alpha, max_iter = 18000)
    lasso.fit(X, y)
    scores.append(lasso.score(X, y))

optimal_alpha = alphas[np.argmax(scores)]
lasso = Lasso(alpha=optimal_alpha)
lasso.fit(X, y)
num_features = np.count_nonzero(lasso.coef_)
print("Minimum number of features for a good model:", num_features)

#Get featur names 
abc = np.where(lasso.coef_ != 0)[0]
feature_names = list(df.columns[abc] )
print(feature_names)

#*****************  Regression for predicting age of Male / Female ***************

#Split into male and female
male = sub_df[sub_df['SEX'] == 1]
female = sub_df[sub_df['SEX'] == 0]

X_male = male.drop(['Age'], axis =1)
y_male = male['Age']

X_female = female.drop(['Age'], axis =1)
y_female = female['Age']

#Male
#spliting
X_train, X_test, y_train, y_test = train_test_split(X_male, y_male, test_size=0.2, random_state=42)

#initiate the model
reg = LinearRegression().fit(X_train, y_train)
reg.score(X_train, y_train)

y_pred = reg.predict(X_test)


# Plot the predicted vs actual values
fig, ax = plt.subplots()
ax.scatter(y_pred, y_test, edgecolors=(0, 0, 0))
ax.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=3)
ax.set_xlabel('Actual')
ax.set_ylabel('Predicted')
ax.set_title('Linear Regression Model for Male')

plt.xlim([0,1])
plt.ylim([-0.1,1])

plt.show()

# Calculation of MSE
mse = mean_squared_error(y_test,y_pred)
print('MSE:',mse)

#Female

#spliting
X_train, X_test, y_train, y_test = train_test_split(X_female, y_female, test_size=0.2, random_state=42)

#initiate the model
reg = LinearRegression().fit(X_train, y_train)
reg.score(X_train, y_train)

y_pred = reg.predict(X_test)


# Plot the predicted vs actual values
fig, ax = plt.subplots()
ax.scatter(y_pred, y_test, edgecolors=(0, 0, 0))
ax.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=3)
ax.set_xlabel('Actual')
ax.set_ylabel('Predicted')
ax.set_title('Linear Regression Model for Female')

plt.xlim([0,1])
plt.ylim([-0.1,1])

plt.show()

# Calculation of MSE
mse = mean_squared_error(y_test,y_pred)
print('MSE:',mse)